# Fletcher AWS F1 design with 4 DDR DIMMs

# Overview

This Fletcher design is based on the CL_DRAM_DMA CustomLogic Example with 1 - 4
DDR controllers. The AxiTop generated by Fletcher is connected to the 2nd AXI
master interface on the interconnect to the DDR controllers (replacing the AXI
master test component), and to the BAR1 AXI-Lite register interface.

1) Register Access over the BAR1 AXI-Lite interface.

2) Mapping of the external four DRAM channel to instance memory via PCIe AppPF
   BAR4, and the 512-bit pcis_dma_ AXI4 bus.

3) Virtual JTAG and Xilinx Integrated Logic Analyzer cores (not tested).

4) User-defined interrupts (not tested).

# Hardware

The hardware design files are stored in the `design` directory. Unless you're
reading this from the `skeleton` directory, your design files should already
have been populated in `design/vhdl` by `project-generate.sh`.

## DRAM Interfaces

The DRAM space is 16 - 64GiB, and is mapped to the sh_cl_dma_pcis AXI4 bus.

By default, only DDR C is used (this one is built into the shell). You can
add DDR banks by modifying the `DDR_x_ABSENT` definitions at the top of
`design/cl_fletcher_aws.sv`.

<a name="dma_pcis"></a>
## dma_pcis AXI4 bus

sh\_cl\_dma\_pcis exposes a address windows of 128GiB matching AppPF BAR4.

This memory space is mapped to the 64GiB DRAM space (the upper half of the
128GiB will just wrap around to the lower half). By default, only DDR
controller C is included, with its 16GiB memory region starting at address
0x8_0000_0000. An AXI crossbar will interleave inbound addresses according
to DDR_A (base_addr=0x0_0000_00000, range=16GB), DDR_B(base_addr=0x4_0000_0000,
range=16GB), DDR_C(base_addr=0x8_0000_0000, range=16GB),
and DDR_D(base_addr=0xC_0000_0000, range=16GB).

Do not attempt to access memory that doesn't exist via DMA; this will hang the
bus and require a hard reset.

## BAR1 AXI-Lite

The sh_cl_bar1\_ AXI-Lite bus is connected to the Fletcher MMIO control
register interface.

## FPGA to FPGA communication over PCIe

This example does not use FPGA to FPGA PCIe communication.

## FPGA to FPGA communication over Ring

This example does not use FPGA to FPGA Ring.

## Virtual JTAG

2 ILA cores are integrated, one to monitoring the sh\_c_dma\_pcis bus and the
other to monitor the AXI4 signals on DDR_A. An example usage is provided in
[cl_ila.sv](design/cl_ila.sv). An example usage for Xilinx VIO is provided in
[cl_vio.sv](design/cl_vio.sv)

Note: this has not been tested by the Fletcher team, it was just copypasted
from the original design.

## Clocks

The design uses the main `clk_main_a0`. This is set to 125MHz by default. You
can change this to 250MHz by adding `-clock_recipe_a A1` to the
`aws_build_dcp_from_cl.sh` synthesis command line.

## Reset

flr_reset is ignored in this design.

# Software

All DMA and BAR1 MMIO accesses are abstracted by the Fletcher runtime libraries
provided [here](../../runtime). Once installed, host applications can just load
a Fletcher platform with the name `aws` to connect to their synthesized kernel.
If you have multiple FPGAs in your system or changed the DDR layout, you'll
have to `#include <fletcher_aws.h>` for the `AwsConfig` structure. Refer to the
comments in the header file for more information.

Cosimulation is a bit unconventional on this platform, since the simulation
executable is the initiator for the simulation, rather than the Fletcher
simulation runtime starting a simulation. The simulation executable will load a
shared object file, which must contain an
`extern "C" void test_main(uint32_t *exit_code)` symbol for the entry point.

Both cases are handled by the example host application in the `software`
directory. The CMake project creates a library for cosimulation (by default this
is `libexample_lib.so`) and an executable (by default named `example`) for
running on hardware. Both entry points lead to `host_main()` in
`software/lib.cpp`, which you can override with your own application logic. The
`argc/argv` for simulation are loaded from the `FLETCHER_AWS_ARGV` environment
variable, which is to be a `:`-separated list of arguments. `argv[0]` (normally
the name of the executable) is always set to `fletcher_aws_sim`.

# Flow

Before running any commands, always source the `sourceme.sh` script at the root
of the `fletcher-aws` repository. Note that its first run will take a bit of
time, because it does a bunch of patching and IP generation for the AWS shell
if this has not been done before. It only takes a few seconds after that.

## Cosimulation

In order to run cosimulation, you need the following things:

 - The Fletcher AWS simulation runtime library needs to be built and installed.
   You can find it at `fletcher-aws/runtime/runtime_sim`.

 - The host application needs to be built, but doesn't need to be installed.
   It's in the `software` directory. The simulation files assume that the
   shared object file will end up at `software/build/lib<name>_lib.so`.

 - The `FLETCHER_AWS_ARGV` environment variable should be set to the desired
   command line arguments for the run, separated by `:` characters.

When you have those, you can run cosimulation using the Makefile in
`verif/scripts` using `make C_TEST=<name>`. Here, `<name>` refers to the tag in
the shared object filename.

## Synthesis

Synthesis is handled by `build/scripts/aws_build_dcp_from_cl.sh`. Useful flags
are `-foreground` and `-clock_recipe_a A1`. The latter selects 250MHz for the
kernel and memory infrastructure, versus the default 125MHz. Once synthesis
completes, a checkpoint tar file will be written to `build/checkpoints/to_aws`.
This file must be written to an S3 bucket. Once there, you can instruct AWS to
run its DRC on the design using `aws ec2 create-fpga-image`.

## Running on hardware

In order to run on hardware, you need the following things:

 - The Fletcher AWS runtime library needs to be built and installed. You can
   find it at `fletcher-aws/runtime/runtime`.

 - The host application needs to be built, but doesn't need to be installed.
   It's in the `software` directory.

 - A suitable FPGA image already loaded onto the FPGA card
   (`sudo fpga-load-local-image -S 0 -I agfi-xxxxxxxxxxxxxxxxx`).

When you have that, you can run the executable generated by the CMake project
in `software`, and, with fingers crossed where appropriate, everything should
work.
